---
description: Python 라이브러리를 활용하여 문서를 처리하고 Ollama 모델과 연동하는 RAG 워크플로우 예시입니다.
---

# \[12] Ollama RAG 실습

## 1. Ollama Python (RAG) 실습

**사전 준비**

```shellscript
pip install langchain-ollama pypdf
```

#### 전체 순서 (Workflow)

_**1단계: PDF 텍스트 추출 (Document Loading)**_

* **내용**: PDF 파일 원본에서 텍스트 데이터를 읽어옵니다.
* **도구**: Python 라이브러리 `pypdf` 등을 사용합니다.

_**2단계: 청킹 (Chunking)**_

* **내용**: 추출한 긴 텍스트를 LLM이 처리하기 적합한 크기(예: 문단 단위, 500자 내외)로 자릅니다.
* **목적**:
  * 검색의 정확도 향상 (관련성 높은 부분만 찾기 위해)
  * 임베딩 효율성 증대
  * LLM의 컨텍스트 윈도우(입력 제한) 준수

_**3단계: 문서 임베딩 (Indexing)**_

* **내용**: 잘린 텍스트 조각(Chunk)들을 임베딩 모델을 통해 컴퓨터가 이해할 수 있는 벡터(숫자 목록)로 변환합니다.
* **저장**: 변환된 벡터를 벡터 저장소(Vector Store)에 저장하여 검색 가능한 상태로 만듭니다.
* **예시 모델**: `text-embedding-004`, `huggingface-embeddings` 등

_**4단계: 질문 임베딩 및 검색 (Retrieval)**_

* **내용**: 사용자의 자연어 질문(예: "이 문서 요약해줘")도 문서와 동일한 방식으로 벡터화합니다.
* **매칭**: 저장된 문서 벡터들 중 질문 벡터와 가장 유사도가 높은(거리가 가까운) 내용을 찾아냅니다.

_**5단계: 답변 생성 (Generation)**_

* **내용**: 검색 단계에서 찾아낸 **관련 내용(Context)**&#xACFC; **사용자의 질문(Query)**&#xC744; 합쳐 프롬프트를 구성합니다.
* **생성**: 이를 LLM에게 보내 문맥에 기반한 최종 답변을 생성합니다.
* **예시 모델**: `gemini-2.5-flash-lite`, `qwen3:0.6b` (Ollama 구동) 등



