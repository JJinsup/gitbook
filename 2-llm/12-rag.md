---
description: Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  Ollama ëª¨ë¸ê³¼ ì—°ë™í•˜ëŠ” RAG ì›Œí¬í”Œë¡œìš° ì˜ˆì‹œì…ë‹ˆë‹¤.
---

# \[12] Ollama RAG ì‹¤ìŠµ

## 1. Ollama Python (RAG) ì‹¤ìŠµ

**ì‚¬ì „ ì¤€ë¹„**

```shellscript
pip install langchain-ollama pypdf
```

#### ì „ì²´ ìˆœì„œ (Workflow)

_**1ë‹¨ê³„: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (Document Loading)**_

* **ë‚´ìš©**: PDF íŒŒì¼ ì›ë³¸ì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.
* **ë„êµ¬**: Python ë¼ì´ë¸ŒëŸ¬ë¦¬ `pypdf` ë“±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

_**2ë‹¨ê³„: ì²­í‚¹ (Chunking)**_

* **ë‚´ìš©**: ì¶”ì¶œí•œ ê¸´ í…ìŠ¤íŠ¸ë¥¼ LLMì´ ì²˜ë¦¬í•˜ê¸° ì í•©í•œ í¬ê¸°(ì˜ˆ: ë¬¸ë‹¨ ë‹¨ìœ„, 500ì ë‚´ì™¸)ë¡œ ìë¦…ë‹ˆë‹¤.
* **ëª©ì **:
  * ê²€ìƒ‰ì˜ ì •í™•ë„ í–¥ìƒ (ê´€ë ¨ì„± ë†’ì€ ë¶€ë¶„ë§Œ ì°¾ê¸° ìœ„í•´)
  * ì„ë² ë”© íš¨ìœ¨ì„± ì¦ëŒ€
  * LLMì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°(ì…ë ¥ ì œí•œ) ì¤€ìˆ˜

_**3ë‹¨ê³„: ë¬¸ì„œ ì„ë² ë”© (Indexing)**_

* **ë‚´ìš©**: ì˜ë¦° í…ìŠ¤íŠ¸ ì¡°ê°(Chunk)ë“¤ì„ ì„ë² ë”© ëª¨ë¸ì„ í†µí•´ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ë²¡í„°(ìˆ«ì ëª©ë¡)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
* **ì €ì¥**: ë³€í™˜ëœ ë²¡í„°ë¥¼ ë²¡í„° ì €ì¥ì†Œ(Vector Store)ì— ì €ì¥í•˜ì—¬ ê²€ìƒ‰ ê°€ëŠ¥í•œ ìƒíƒœë¡œ ë§Œë“­ë‹ˆë‹¤.
* **ì˜ˆì‹œ ëª¨ë¸**: `text-embedding-004`, `huggingface-embeddings` ë“±

_**4ë‹¨ê³„: ì§ˆë¬¸ ì„ë² ë”© ë° ê²€ìƒ‰ (Retrieval)**_

* **ë‚´ìš©**: ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸(ì˜ˆ: "ì´ ë¬¸ì„œ ìš”ì•½í•´ì¤˜")ë„ ë¬¸ì„œì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë²¡í„°í™”í•©ë‹ˆë‹¤.
* **ë§¤ì¹­**: ì €ì¥ëœ ë¬¸ì„œ ë²¡í„°ë“¤ ì¤‘ ì§ˆë¬¸ ë²¡í„°ì™€ ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€(ê±°ë¦¬ê°€ ê°€ê¹Œìš´) ë‚´ìš©ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.

_**5ë‹¨ê³„: ë‹µë³€ ìƒì„± (Generation)**_

* **ë‚´ìš©**: ê²€ìƒ‰ ë‹¨ê³„ì—ì„œ ì°¾ì•„ë‚¸ **ê´€ë ¨ ë‚´ìš©(Context)**&#xACFC; **ì‚¬ìš©ìì˜ ì§ˆë¬¸(Query)**&#xC744; í•©ì³ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
* **ìƒì„±**: ì´ë¥¼ LLMì—ê²Œ ë³´ë‚´ ë¬¸ë§¥ì— ê¸°ë°˜í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
* **ì˜ˆì‹œ ëª¨ë¸**: `gemini-2.5-flash-lite`, `qwen3:0.6b` (Ollama êµ¬ë™) ë“±

### 3. ì‹¤ìŠµ 1: LLMì„ í•¨ìˆ˜ì²˜ëŸ¼ ì‚¬ìš©í•˜ê¸° (Structured Output)

ì´ ì‹¤ìŠµì—ì„œëŠ” **Ollama**ë¡œ êµ¬ë™ë˜ëŠ” `qwen3:0.6b` ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬, ë¹„ì •í˜• ìì—°ì–´ í…ìŠ¤íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ ì •í˜•í™”ëœ JSON í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

#### 3.1 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ëª¨ë¸ ì„¤ì •

ë¨¼ì € í•„ìš”í•œ ë„êµ¬ë“¤ì„ ë¶ˆëŸ¬ì˜¤ê³ , ë¡œì»¬ì— ì„¤ì¹˜ëœ Ollama ëª¨ë¸ì„ ì—°ê²°í•©ë‹ˆë‹¤.

```
import os
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ëª¨ë¸ ì´ˆê¸°í™”
# model="qwen3:0.6b": Ollamaì— ì„¤ì¹˜ëœ ëª¨ë¸ ì´ë¦„ì„ ì •í™•íˆ ì…ë ¥í•©ë‹ˆë‹¤.
# temperature=0: ì°½ì˜ì„±ì„ 0ìœ¼ë¡œ ë‚®ì¶”ì–´, í•­ìƒ ì¼ê´€ë˜ê³  ì‚¬ì‹¤ì ì¸ ë‹µë³€ë§Œ í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤. (ë°ì´í„° ì¶”ì¶œì— í•„ìˆ˜)
llm = ChatOllama(model="qwen3:0.6b", temperature=0)
```

#### 3.2 í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„± (Few-shot Learning)

ì‘ì€ ëª¨ë¸ì¼ìˆ˜ë¡ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ **Few-shot Prompting**ì´ë¼ê³  í•©ë‹ˆë‹¤. ëª¨ë¸ì—ê²Œ "ì…ë ¥ì´ Aì¼ ë•Œ ì¶œë ¥ì€ Bì—¬ì•¼ í•´"ë¼ê³  ê°€ë¥´ì³ì¤ë‹ˆë‹¤.

```
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
template = """
You are a precise Data Extraction Module.
Extract hardware status from the input text into a JSON List format.

Examples:
Input: "Battery is 12V and Camera is OK."
Output: [
    {{"component": "Battery", "status": "Normal", "value": 12}},
    {{"component": "Camera", "status": "OK", "value": null}}
]

Input: {input}
Output:
"""

prompt = ChatPromptTemplate.from_template(template)
```

#### 3.3 ì²´ì¸ ìƒì„± ë° ì‹¤í–‰

LangChainì˜ íŒŒì´í”„(`|`) ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ `í”„ë¡¬í”„íŠ¸ -> ëª¨ë¸ -> ì¶œë ¥ íŒŒì„œ` ìˆœì„œë¡œ ì—°ê²°í•©ë‹ˆë‹¤.

```
# ì²´ì¸(Chain) ì—°ê²°
# 1. prompt: ì‚¬ìš©ìì˜ ì…ë ¥ì„ í…œí”Œë¦¿ì— ì±„ì›€
# 2. llm: ì±„ì›Œì§„ í”„ë¡¬í”„íŠ¸ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±
# 3. StrOutputParser: ëª¨ë¸ì˜ ë‹µë³€ ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ê¹”ë”í•˜ê²Œ ì¶”ì¶œ
chain = prompt | llm | StrOutputParser()

# í…ŒìŠ¤íŠ¸ ì…ë ¥ ë°ì´í„°
user_input = "LiDAR ì„¼ì„œ-> ì •ìƒ ì‘ë™ ì¤‘, ëª¨í„° ì˜¨ë„ê°€ 45ë„ë¼ì„œ ê²½ê³  ìƒíƒœì•¼."

# ì²´ì¸ ì‹¤í–‰ (Invoke)
print(f"ì…ë ¥: {user_input}")
response = chain.invoke({"input": user_input})

print("ì¶œë ¥ ê²°ê³¼:")
print(response)
```

> _**ì‹¤í–‰ê²°ê³¼:**_\
> **\[**\
> **{"component": "LiDAR", "status": "Normal", "value": null},**\
> **{"component": "Motor", "status": "Warning", "value": 45}**\
> **]**

### 4. ì‹¤ìŠµ 2: ë¬¸ì„œ ê¸°ë°˜ ì „ë¬¸ê°€ (On-Device RAG)

ì´ ì‹¤ìŠµì—ì„œëŠ” PDF ë¬¸ì„œë¥¼ ì½ì–´ ë²¡í„°í™”(Embedding)í•˜ê³ , ì´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€í•˜ëŠ” ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. ë‹¨ê³„ë³„ë¡œ ì½”ë“œë¥¼ ë‚˜ëˆ„ì–´ ì§„í–‰í•©ë‹ˆë‹¤.

###

### 4.1 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ê¸°ë³¸ ì„¤ì •

#### 4.1.1 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

```python
import os
import numpy as np
from dotenv import load_dotenv
from google import genai
from google.genai import types
from pypdf import PdfReader
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
```

#### 4.1.2 í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° API í‚¤ í™•ì¸

```python
# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°)
load_dotenv()
api_key = os.environ.get("GEMINI_API_KEY")

if not api_key:
    print("ì˜¤ë¥˜: GEMINI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit()
```

#### 4.1.3 Gemini í´ë¼ì´ì–¸íŠ¸ ë° ë¡œì»¬ LLM ì´ˆê¸°í™”

```python
# 2. Google Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì„ë² ë”© ìƒì„±ìš©)
client = genai.Client(api_key=api_key)

# 3. ë¡œì»¬ LLM ì´ˆê¸°í™” (ë‹µë³€ ìƒì„±ìš©)
# temperature=0: ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì„ ìœ„í•´ ì°½ì˜ì„±ì„ ë‚®ì¶¤
llm = ChatOllama(model="qwen3:0.6b", temperature=0)
```

***

### 4.2 í•µì‹¬ í•¨ìˆ˜ 1: í…ìŠ¤íŠ¸ ì„ë² ë”©(Vectorization)

#### 4.2.1 ì„ë² ë”© ê°œë…

* í…ìŠ¤íŠ¸ â†’ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜
* ë¬¸ì„œ ì €ì¥ìš©: `RETRIEVAL_DOCUMENT`
* ì§ˆë¬¸ ê²€ìƒ‰ìš©: `RETRIEVAL_QUERY`

#### 4.2.2 ì„ë² ë”© í•¨ìˆ˜ êµ¬í˜„

```python
def get_embedding(text, task_type="RETRIEVAL_DOCUMENT"):
    """
    Google Gemini ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - task_type: 'RETRIEVAL_DOCUMENT'(ë¬¸ì„œ ì €ì¥ìš©) ë˜ëŠ” 'RETRIEVAL_QUERY'(ê²€ìƒ‰ ì§ˆë¬¸ìš©)
    """
    try:
        response = client.models.embed_content(
            model="text-embedding-004",
            contents=text,
            config=types.EmbedContentConfig(task_type=task_type)
        )
        return response.embeddings[0].values
    except Exception as e:
        print(f"ì„ë² ë”© ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []
```

***

### 4.3 í•µì‹¬ í•¨ìˆ˜ 2: ìœ ì‚¬ë„ ê²€ìƒ‰ (Retrieval)

#### 4.3.1 ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰

* ì§ˆë¬¸ ë²¡í„° vs ë¬¸ì„œ ë²¡í„° ê°„ ë‚´ì (dot product) ì‚¬ìš©
* ê°’ì´ í´ìˆ˜ë¡ ì˜ë¯¸ì ìœ¼ë¡œ ë” ìœ ì‚¬

#### 4.3.2 ê²€ìƒ‰ í•¨ìˆ˜ êµ¬í˜„

```python
def search_pdf(query, chunks, chunk_embeddings):
    """
    ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    """
    # 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜ (ê²€ìƒ‰ ì¿¼ë¦¬ìš© ëª¨ë“œ ì‚¬ìš©)
    query_vec = get_embedding(query, task_type="RETRIEVAL_QUERY")
    
    if not query_vec:
        return "ê²€ìƒ‰ ì˜¤ë¥˜"

    # 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì§ˆë¬¸ ë²¡í„° vs ëª¨ë“  ë¬¸ì„œ ë²¡í„° ë‚´ì )
    # ê°’ì´ í´ìˆ˜ë¡ ìœ ì‚¬ë„ê°€ ë†’ìŒ
    scores = [np.dot(query_vec, doc_vec) for doc_vec in chunk_embeddings]
    
    # 3. ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ì¸ë±ìŠ¤ ì°¾ê¸°
    best_idx = np.argmax(scores)
    
    return chunks[best_idx]
```

***

### 4.4 ë°ì´í„° ì¤€ë¹„: PDF ë¡œë”© ë° ë²¡í„° DB êµ¬ì¶•

#### 4.4.1 PDF íŒŒì¼ ë¡œë”© ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ

```python
pdf_path = "src/2025_vla.pdf"  # ë¶„ì„í•  PDF íŒŒì¼ ê²½ë¡œ

try:
    print(f"--- [ë°ì´í„° ì¤€ë¹„ ì‹œì‘] ---")
    print(f"1. PDF '{pdf_path}' ë¡œë”© ì¤‘...")
    reader = PdfReader(pdf_path)
    
    # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” í˜ì´ì§€ë§Œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
    chunks = [page.extract_text() for page in reader.pages if page.extract_text()]
    
    if not chunks:
        print("ê²½ê³ : PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ë§Œ ìˆëŠ” PDFì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        exit()
```

#### 4.4.2 í˜ì´ì§€ ë‹¨ìœ„ ì„ë² ë”© ìƒì„±

```python
    print(f"2. {len(chunks)}ê°œ í˜ì´ì§€ ë²¡í„°í™” ì§„í–‰ ì¤‘... (ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    # ëª¨ë“  í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ì„ë² ë”© ìƒì„±
    chunk_embeddings = [get_embedding(chunk) for chunk in chunks]
    print("   -> ë²¡í„°í™” ì™„ë£Œ! ê²€ìƒ‰ ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤.\n")

except FileNotFoundError:
    print(f"ì˜¤ë¥˜: '{pdf_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit()
```

***

### 4.5 ì‹¤ì „ í…ŒìŠ¤íŠ¸: RAG ì„±ëŠ¥ ë¹„êµ

ã„±

1. ì¼ë°˜ LLMë§Œ ì¼ì„ ë•Œ
2. RAGë¥¼ ì ìš©í–ˆì„ ë•Œ

#### 4.6.1 í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì •ì˜

```python
test_question = "ì´ë²ˆ VLA íŠ¹ê°•ì˜ ì‹¤ìŠµ ì¡°êµ(TA) ì´ë¦„ê³¼ ë¬¸ì˜ ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì•Œë ¤ì¤˜."
```

***

#### 4.6.2 Case 1: RAG ë¯¸ì ìš© (ì¼ë°˜ LLM)

```python
print(f"--- [âŒ Case 1: RAG ë¯¸ì ìš© (ì¼ë°˜ LLM)] ---")
print("ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì„ ì§ˆë¬¸ë°›ì•˜ì„ ë•Œ:")

template_no_rag = "ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.\nì§ˆë¬¸: {question}"
prompt_no_rag = ChatPromptTemplate.from_template(template_no_rag)
chain_no_rag = prompt_no_rag | llm | StrOutputParser()

# ëª¨ë¸ì´ í• ë£¨ì‹œë„¤ì´ì…˜(ê±°ì§“ë§)ì„ í•˜ê±°ë‚˜ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ëŠ”ì§€ í™•ì¸
response_before = chain_no_rag.invoke({"question": test_question})
print(f"ë‹µë³€:\n{response_before}\n")
```

***

#### 4.6.3 Case 2: RAG ì ìš© (ê²€ìƒ‰ + LLM)

**4.6.3.1 ë¬¸ì„œ ê²€ìƒ‰ (Retrieval)**

```python
print(f"--- [â­• Case 2: RAG ì ìš© (ë¬¸ì„œ ê²€ìƒ‰ + LLM)] ---")
print("ë¬¸ì„œì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì•„ì„œ ë‹µë³€í•  ë•Œ:")

# 1. ê²€ìƒ‰ (Retrieval): ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš© ì°¾ì•„ì˜¤ê¸°
found_context = search_pdf(test_question, chunks, chunk_embeddings)
print(f"ğŸ” [ê²€ìƒ‰ëœ ê·¼ê±° ìë£Œ(Context)]:\n...{found_context[:150]}...\n")
```

**4.6.3.2 í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Augmentation)**

```python
rag_template = """
You are a helpful assistant. Answer the question based ONLY on the context below.
If the answer is not in the context, say "I don't know".

Context:
{context}

Question:
{question}

Answer:
"""
rag_prompt = ChatPromptTemplate.from_template(rag_template)
```

**4.6.3.3 ìµœì¢… ìƒì„± (Generation)**

```python
# 3. ìƒì„± (Generation): ì •í™•í•œ ë‹µë³€ ìƒì„±
rag_chain = rag_prompt | llm | StrOutputParser()
response_after = rag_chain.invoke({
    "context": found_context,   # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©
    "question": test_question   # ì‚¬ìš©ì ì§ˆë¬¸
})

print(f"ìµœì¢… ë‹µë³€:\n{response_after}")
```
