---
description: >-
  Unsloth ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì´ˆê²½ëŸ‰ LLMì¸ Qwen 0.6B ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒì¸íŠœë‹(Fine-tuning)í•˜ëŠ” ì „ì²´ ê³¼ì •ì„
  ë‹¤ë£¹ë‹ˆë‹¤.
---

# ğŸ“š \[14] ë¡œì»¬ LLM íŒŒì¸íŠœë‹&#x20;

{% embed url="https://cloud.google.com/use-cases/fine-tuning-ai-models?hl=ko" %}

### 0. íŒŒì¸ íŠœë‹(Fine-Tuning)ì´ë€?

ë°©ëŒ€í•œ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµ(Pre-training)ëœ LLMì„ **íŠ¹ì • ì‘ì—…ì´ë‚˜ ë„ë©”ì¸ì— ë§ëŠ” ë°ì´í„°ì…‹ìœ¼ë¡œ ì¶”ê°€ í•™ìŠµ**ì‹œí‚¤ëŠ” ê³¼ì •(ì „ì´ í•™ìŠµ)ì…ë‹ˆë‹¤.

ì‰½ê²Œ ë§í•´, ì´ë¯¸ ì–¸ì–´ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ê°–ì¶˜ 'ë˜‘ë˜‘í•œ ì‹ ì…ì‚¬ì›'(ì‚¬ì „ í•™ìŠµ ëª¨ë¸)ì—ê²Œ "ìš°ë¦¬ íšŒì‚¬ì˜ ì „ë¬¸ ìš©ì–´"ë‚˜ "íŠ¹ì • ì—…ë¬´ ë§¤ë‰´ì–¼"ì„ ê°€ë¥´ì³ 'í•´ë‹¹ ë¶„ì•¼ì˜ ì „ë¬¸ê°€'ë¡œ ë§Œë“œëŠ” ê³¼ì •ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ğŸ’¡ íŒŒì¸ íŠœë‹ì´ í•„ìš”í•œ ìˆœê°„

ë‹¨ìˆœíˆ í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ ì“°ëŠ” ê²ƒë³´ë‹¤ íŒŒì¸ íŠœë‹ì„ í•´ì•¼ í•  ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

* **ğŸ—£ï¸ ì „ë¬¸ ìš©ì–´/ìŠ¤íƒ€ì¼ ìŠµë“**: ì—…ê³„ ì „ìš© ì€ì–´, ì‚¬ë‚´ ì•½ì–´, í˜¹ì€ íŠ¹ì • ë¸Œëœë“œì˜ ì–´ì¡°(Tone & Manner)ë¥¼ ëª¨ë¸ì´ êµ¬ì‚¬í•´ì•¼ í•  ë•Œ.
* **âš¡ ë¹„ìš© ë° ì†ë„ ìµœì í™”**: ë§¤ë²ˆ ê¸´ í”„ë¡¬í”„íŠ¸ë¥¼ ë„£ê±°ë‚˜ ê±°ëŒ€ ëª¨ë¸(GPT-4 ë“±)ì„ ì“°ëŠ” ëŒ€ì‹ , ì‘ì€ ëª¨ë¸ì„ íŠœë‹í•˜ì—¬ ì €ë¹„ìš©Â·ì €ì§€ì—°(Latency)ìœ¼ë¡œ ìš´ì˜í•˜ê³  ì‹¶ì„ ë•Œ.
* **ğŸ¯ ì •í™•ë„ í–¥ìƒ**: ê°ì„± ë¶„ì„, ì˜ë£Œ ì°¨íŠ¸ ë¶„ë¥˜ ë“± íŠ¹ì • íƒœìŠ¤í¬ì—ì„œ ë²”ìš© ëª¨ë¸ë³´ë‹¤ ë” ë†’ì€ ì •í™•ë„ê°€ í•„ìš”í•  ë•Œ.
* **ğŸ§© íŠ¹ì´ ì‚¬ë¡€(Edge Case) ì²˜ë¦¬**: í”„ë¡¬í”„íŠ¸ë§Œìœ¼ë¡œëŠ” ì„¤ëª…í•˜ê¸° í˜ë“  ë³µì¡í•œ ê·œì¹™ì´ë‚˜ ì˜ˆì™¸ ìƒí™©ì„ ëª¨ë¸ì´ ì´í•´í•´ì•¼ í•  ë•Œ.

#### ğŸ”„ íŒŒì¸ íŠœë‹ ì§„í–‰ 4ë‹¨ê³„

1. **ë°ì´í„° ì¤€ë¹„**: ê³ í’ˆì§ˆì˜ ì‘ì—…ë³„ ë°ì´í„°ì…‹ì„ ìˆ˜ì§‘í•˜ê³  ì •ì œí•©ë‹ˆë‹¤. (ë°ì´í„° í’ˆì§ˆì´ ê°€ì¥ ì¤‘ìš”!)
2. **ì ‘ê·¼ ë°©ì‹ ì„ íƒ**:
   * **ì „ì²´ íŒŒì¸ íŠœë‹**: ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ (ë¹„ìš© ë†’ìŒ).
   * **PEFT (Parameter-Efficient Fine-Tuning)**: ëª¨ë¸ì˜ ëŒ€ë¶€ë¶„ì„ ê³ ì •í•˜ê³  **í•µì‹¬ íŒŒë¼ë¯¸í„°ë§Œ ì¶”ê°€ í•™ìŠµ**í•˜ëŠ” íš¨ìœ¨ì  ë°©ì‹. (ë³¸ ê°€ì´ë“œì—ì„œ ì‚¬ìš©í•  **LoRA**ê°€ ì—¬ê¸°ì— í•´ë‹¹)
3. **ëª¨ë¸ í•™ìŠµ**: ì¤€ë¹„ëœ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµ(Train)ì‹œí‚¤ë©° í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.
4. **í‰ê°€ ë° ë°°í¬**: í•™ìŠµëœ ëª¨ë¸ì´ ìƒˆë¡œìš´ ë°ì´í„°(Test set)ì—ë„ ì˜ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦í•˜ê³  ì‹¤ì„œë¹„ìŠ¤ì— ë°°í¬í•©ë‹ˆë‹¤.

> **âš ï¸ ì£¼ì˜ì‚¬í•­ (Challenges)**
>
> * **ê³¼ì í•©(Overfitting)**: ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ë§Œ ë‹¬ë‹¬ ì™¸ì›Œì„œ, ì¡°ê¸ˆë§Œ ë‹¤ë¥¸ ì§ˆë¬¸ì—ëŠ” ëŒ€ë‹µí•˜ì§€ ëª»í•˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤.
> * **ì¹˜ëª…ì  ë§ê°(Catastrophic Forgetting)**: ìƒˆë¡œìš´ ì „ë¬¸ ì§€ì‹ì„ ë°°ìš°ëŠë¼ ê¸°ì¡´ì˜ ì¼ë°˜ ìƒì‹(ê¸°ë³¸ ì–¸ì–´ ëŠ¥ë ¥ ë“±)ì„ ìŠì–´ë²„ë¦¬ëŠ” í˜„ìƒì…ë‹ˆë‹¤.

#### ğŸ”„ íŒŒì¸ íŠœë‹ ì§„í–‰ 4ë‹¨ê³„

1. **ë°ì´í„° ì¤€ë¹„**: ê³ í’ˆì§ˆì˜ ì‘ì—…ë³„ ë°ì´í„°ì…‹ì„ ìˆ˜ì§‘í•˜ê³  ì •ì œí•©ë‹ˆë‹¤. (ë°ì´í„° í’ˆì§ˆì´ ê°€ì¥ ì¤‘ìš”!)
2. **ì ‘ê·¼ ë°©ì‹ ì„ íƒ**:
   * **ì „ì²´ íŒŒì¸ íŠœë‹**: ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ (ë¹„ìš© ë†’ìŒ).
   * **PEFT (Parameter-Efficient Fine-Tuning)**: ëª¨ë¸ì˜ ëŒ€ë¶€ë¶„ì„ ê³ ì •í•˜ê³  **í•µì‹¬ íŒŒë¼ë¯¸í„°ë§Œ ì¶”ê°€ í•™ìŠµ**í•˜ëŠ” íš¨ìœ¨ì  ë°©ì‹. (ë³¸ ê°€ì´ë“œì—ì„œ ì‚¬ìš©í•  **LoRA**ê°€ ì—¬ê¸°ì— í•´ë‹¹)
3. **ëª¨ë¸ í•™ìŠµ**: ì¤€ë¹„ëœ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµ(Train)ì‹œí‚¤ë©° í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.
4. **í‰ê°€ ë° ë°°í¬**: í•™ìŠµëœ ëª¨ë¸ì´ ìƒˆë¡œìš´ ë°ì´í„°(Test set)ì—ë„ ì˜ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦í•˜ê³  ì‹¤ì„œë¹„ìŠ¤ì— ë°°í¬í•©ë‹ˆë‹¤.

###

> **âš ï¸ ì£¼ì˜ì‚¬í•­ (Challenges)**
>
> * **ê³¼ì í•©(Overfitting)**: ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ë§Œ ë‹¬ë‹¬ ì™¸ì›Œì„œ, ì¡°ê¸ˆë§Œ ë‹¤ë¥¸ ì§ˆë¬¸ì—ëŠ” ëŒ€ë‹µí•˜ì§€ ëª»í•˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤.
> * **ì¹˜ëª…ì  ë§ê°(Catastrophic Forgetting)**: ìƒˆë¡œìš´ ì „ë¬¸ ì§€ì‹ì„ ë°°ìš°ëŠë¼ ê¸°ì¡´ì˜ ì¼ë°˜ ìƒì‹(ê¸°ë³¸ ì–¸ì–´ ëŠ¥ë ¥ ë“±)ì„ ìŠì–´ë²„ë¦¬ëŠ” í˜„ìƒì…ë‹ˆë‹¤.

### 1. íŒŒì¸íŠœë‹ í™˜ê²½ ì¤€ë¹„ (Setup)

ê°€ì¥ ë¨¼ì € í•™ìŠµì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. UnslothëŠ” ìµœì í™”ëœ ì»¤ë„ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— íŠ¹ì • ë²„ì „ì˜ ì˜ì¡´ì„±ì„ ë§ì¶°ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.

#### 1.1 í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```
# Unsloth ë° ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install unsloth
pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes
```

#### 1.2 ì¶”ê°€ ì˜ì¡´ì„± ì„¤ì¹˜ (CMake/Curl)

UnslothëŠ” ë‚´ë¶€ì ìœ¼ë¡œ C++ ì»´íŒŒì¼ì´ í•„ìš”í•œ ê¸°ëŠ¥ì´ë‚˜ GGUF ë³€í™˜ ë“±ì„ ì§€ì›í•˜ê¸° ìœ„í•´ ì‹œìŠ¤í…œ ë ˆë²¨ì˜ ë„êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

```
# Ubuntu/Colab ê¸°ì¤€
sudo apt-get update
sudo apt-get install libcurl4-openssl-dev cmake make -y
```

> **ğŸ’¡ Info** `xformers`ëŠ” ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ì—°ì‚°ì„ ì§€ì›í•˜ë©°, `trl`ì€ ê°•í™”í•™ìŠµ ë° SFT(Supervised Fine-Tuning)ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

### 2. ëª¨ë¸ ë¡œë“œ ë° LoRA êµ¬ì„±

ì´ì œ ë² ì´ìŠ¤ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³ , íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•´ \*\*LoRA(Low-Rank Adaptation)\*\*ë¥¼ ì ìš©í•©ë‹ˆë‹¤. ì•ì„œ ì„¤ëª…í•œ **PEFT** ê¸°ë²•ì˜ ì¼ì¢…ì…ë‹ˆë‹¤.

#### 2.1 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì„¤ì •

```
import torch
from unsloth import FastLanguageModel
from datasets import load_dataset

# ëª¨ë¸ ì„¤ì •
# (ì˜ˆì‹œ: Qwen2.5-0.5B ë“±ì˜ ì‹¤ì œ ëª¨ë¸ëª… ì‚¬ìš© ê¶Œì¥)
model_name = "unsloth/Qwen2.5-0.5B-unsloth-bnb-4bit" 
max_seq_length = 2048   # ì…ë ¥ ë¬¸ë§¥ ìµœëŒ€ ê¸¸ì´
dtype = None            # Noneìœ¼ë¡œ ì„¤ì • ì‹œ ìë™ ê°ì§€ (Float16 ë“±)
load_in_4bit = True     # 4bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
```

#### 2.2 ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ

`FastLanguageModel`ì„ ì‚¬ìš©í•˜ë©´ ì¼ë°˜ì ì¸ HuggingFace `AutoModel`ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê²Œ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)
```

#### 2.3 LoRA ì–´ëŒ‘í„° ë¶€ì°©

ì „ì²´ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ëŠ” ëŒ€ì‹ , **LoRA** ì–´ëŒ‘í„°ë¥¼ ë¶™ì—¬ ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµí•©ë‹ˆë‹¤. ì´ëŠ” í•™ìŠµ ì†ë„ë¥¼ ë¹„ì•½ì ìœ¼ë¡œ ë†’ì—¬ì¤ë‹ˆë‹¤.

```
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,             # LoRA Rank (ë†’ì„ìˆ˜ë¡ í‘œí˜„ë ¥ ì¦ê°€, ë©”ëª¨ë¦¬ ì¦ê°€)
    target_modules = [  # í•™ìŠµì‹œí‚¬ ëª¨ë“ˆ ì§€ì • (ëª¨ë“  ì„ í˜• ë ˆì´ì–´ ê¶Œì¥)
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_alpha = 16,    # LoRA ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜
    lora_dropout = 0,   # 0 ê¶Œì¥ (ìµœì í™” ë¬¸ì œ ë°©ì§€)
    bias = "none",
    use_gradient_checkpointing = "unsloth", # ë©”ëª¨ë¦¬ ì ˆì•½ ê¸°ìˆ 
    random_state = 3407,
)
```

### 3. ë°ì´í„°ì…‹ ì¤€ë¹„

í•™ìŠµí•  ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” **Chat Template** í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

#### 3.1 ë°ì´í„° ë¡œë“œ

```
# JSONL íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
dataset = load_dataset("json", data_files="6g_ai_dataset_augmented.jsonl", split="train")
```

#### 3.2 í¬ë§· ë³€í™˜ (Formatting)

Qwen ëª¨ë¸ì€ ëŒ€í™”í˜• ëª¨ë¸ì´ë¯€ë¡œ, `Instruction`(ì§€ì‹œ)ê³¼ `Output`(ë‹µë³€) ìŒì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.

```
def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    outputs = examples["output"]
    texts = []
    
    for instruction, output in zip(instructions, outputs):
        # Qwen ì±„íŒ… í…œí”Œë¦¿ êµ¬ì¡° ì •ì˜
        messages = [
            {"role": "user", "content": instruction},
            {"role": "assistant", "content": output}
        ]
        
        # í† í¬ë‚˜ì´ì €ë¥¼ í†µí•´ í…ìŠ¤íŠ¸í™” (EOS í† í° ë“± ìë™ ì²˜ë¦¬)
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        texts.append(text)
        
    return {"text": texts}

# ë°ì´í„°ì…‹ì— ë³€í™˜ ì ìš©
train_dataset = dataset.map(formatting_prompts_func, batched=True)

# ë³€í™˜ ê²°ê³¼ í™•ì¸
print(train_dataset[0]["text"])
```

### 4. í•™ìŠµ(Training) ì‹¤í–‰

ì¤€ë¹„ëœ ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ `SFTTrainer`ì— ì—°ê²°í•˜ì—¬ ë³¸ê²©ì ì¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.

#### 4.1 Trainer ì„¤ì •

```
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train_dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    args = TrainingArguments(
        per_device_train_batch_size = 2,  # ë°°ì¹˜ ì‚¬ì´ì¦ˆ
        gradient_accumulation_steps = 4,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  (ì‹¤ì œ ë°°ì¹˜ 2*4=8 íš¨ê³¼)
        warmup_steps = 5,
        max_steps = 60,                   # ì´ í•™ìŠµ ìŠ¤í… (ë°ì´í„° ì–‘ì— ë”°ë¼ ì¡°ì ˆ)
        learning_rate = 2e-4,             # í•™ìŠµë¥ 
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),   # Ampere GPU ì´ìƒì´ë©´ bf16 ì‚¬ìš©
        logging_steps = 1,
        optim = "adamw_8bit",             # 8bit ì˜µí‹°ë§ˆì´ì €ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)
```

#### 4.2 í•™ìŠµ ì‹œì‘

```
trainer_stats = trainer.train()
```

> **ğŸš€ Tip** `max_steps`ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤. ë°ì´í„°ì…‹ì´ í¬ë‹¤ë©´ `num_train_epochs`ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ `max_steps`ë¥¼ ëŠ˜ë¦¬ì„¸ìš”.

### 5. ëª¨ë¸ í…ŒìŠ¤íŠ¸ (Inference)

í•™ìŠµì´ ëë‚œ ëª¨ë¸ì´ ì§ˆë¬¸ì— ì˜¬ë°”ë¥´ê²Œ ë‹µë³€í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

```
from transformers import TextStreamer

# 1. ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜ (ì†ë„ ìµœì í™”)
FastLanguageModel.for_inference(model)

# 2. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì‘ì„±
messages = [
    {"role": "user", "content": "6G ë„¤íŠ¸ì›Œí¬ì—ì„œ AIì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?"}
]

# 3. ì…ë ¥ í† í°í™”
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt",
).to("cuda")

# 4. ë‹µë³€ ìƒì„± ë° ì¶œë ¥
_ = model.generate(
    input_ids=inputs,
    streamer=TextStreamer(tokenizer, skip_prompt=True),
    max_new_tokens=256,
    use_cache=True,
    temperature=0.7,
)
```

### 6. ëª¨ë¸ ì €ì¥ (Export)

í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ì™€ ì„¤ì •ì„ ì €ì¥í•©ë‹ˆë‹¤. GGUF ë³€í™˜ì€ í™˜ê²½ì— ë”°ë¼ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê°€ì¥ í˜¸í™˜ì„±ì´ ì¢‹ì€ **HuggingFace í¬ë§·** ì €ì¥ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

```
save_dir = "outputs/"

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {save_dir}")
```

### 7. ë¡œë“œ ë° ê²€ì¦ (Reload Test)

ì„¸ì…˜ì„ ì¬ì‹œì‘í–ˆì„ ë•Œ, ì €ì¥ëœ ëª¨ë¸ì„ ì •ìƒì ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ”ì§€ ê²€ì¦í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.

```
from unsloth import FastLanguageModel

save_dir = "outputs/"

# 1. ì €ì¥ëœ ê²½ë¡œì—ì„œ ëª¨ë¸ ë¡œë“œ
loaded_model, loaded_tokenizer = FastLanguageModel.from_pretrained(
    model_name = save_dir, # ì €ì¥ëœ í´ë” ê²½ë¡œ ì§€ì •
    max_seq_length = 2048,
    dtype = None,
    load_in_4bit = True,
)

# 2. ì¶”ë¡  ëª¨ë“œ ì „í™˜
FastLanguageModel.for_inference(loaded_model)

# 3. ì¬ê²€ì¦
messages = [{"role": "user", "content": "6G ë„¤íŠ¸ì›Œí¬ì—ì„œ AIì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?"}]
inputs = loaded_tokenizer.apply_chat_template(
    messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
).to("cuda")

output = loaded_model.generate(input_ids=inputs, max_new_tokens=256)
print(loaded_tokenizer.decode(output[0], skip_special_tokens=True))
```

### 8. ìš”ì•½ ë° í™œìš© (Summary)

ì´ ê°€ì´ë“œë¥¼ í†µí•´ ìš°ë¦¬ëŠ” ë‹¤ìŒ ê³¼ì •ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤:

1. **Unsloth í™˜ê²½ êµ¬ì¶•**: Qwen 0.6B/0.5B ëª¨ë¸ì„ 4bitë¡œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ í™•ë³´
2. **ë°ì´í„°ì…‹ ë³€í™˜**: Raw JSONL ë°ì´í„°ë¥¼ LLMì´ ì´í•´í•˜ëŠ” Chat Formatìœ¼ë¡œ ë³€í™˜
3. **LoRA íŒŒì¸íŠœë‹**: ì ì€ ë¦¬ì†ŒìŠ¤ë¡œ ëª¨ë¸ì˜ ì§€ì‹ì„ íŠ¹ì • ë„ë©”ì¸(6G, AI ë“±)ì— ë§ê²Œ íŠœë‹
4. **ì €ì¥ ë° ê²€ì¦**: í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•˜ê³  ë‹¤ì‹œ ë¶ˆëŸ¬ì™€ ì¶”ë¡  í…ŒìŠ¤íŠ¸

ì´ ì›Œí¬í”Œë¡œìš°ëŠ” **ì‚¬ë‚´ êµ¬ì¶•í˜•(On-Premise) LLM**, **ë¡œë´‡ ì œì–´ìš© ê²½ëŸ‰ ëª¨ë¸**, **ëª¨ë°”ì¼ ì˜¨ë””ë°”ì´ìŠ¤ AI** ë“±ì„ ê°œë°œí•  ë•Œ í•µì‹¬ì ì¸ ê¸°ì´ˆê°€ ë©ë‹ˆë‹¤.
