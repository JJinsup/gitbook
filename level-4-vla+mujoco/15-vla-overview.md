---
description: From “Sim2Real” to Physical AI
---

# \[15] VLA Overview

### 1. 개요

VLA 모델(Vision-Language-Action Model)은 텍스트, 이미지·비디오, 시연(Demonstration) 등의 입력을 받아 로봇의 액션을 직접 생성하는 로봇 파운데이션 모델을 통칭하는 개념입니다.

즉, 로봇 내부에서 **인지** → 추론 **→ 행동 생성**을 하나의 모델 또는 하나의 통합 파이프라인으로 수행하는 생성형 인공지능 로봇 모델이라 볼 수 있습니다.

기존의 로봇 제어 시스템이 아래와 같이 개별 모듈로 나누어 설계했다면,

* 센서 처리
* 상태 추정
* 경로 계획
* 제어기 설계

VLA는 이 중 상당 부분을 대규모 신경망 기반 모델이 통합적으로 담당한다는 점에서 근본적인 차이를 가집니다.

> **\[이미지 삽입 구간]**
>
> **Vision–Language–Action 파이프라인 개념도** (입력(이미지·텍스트) → 추론 → 로봇 행동 흐름 다이어그램)

### 2. VLA와 로봇 파운데이션 모델의 관계

VLA는 \*\*로봇 파운데이션 모델(Robot Foundation Model)\*\*의 **부분집합**에 해당합니다.

* **VLA:** 반드시 Vision, Language, Action이라는 세 가지 컴포넌트를 모두 포함해야 합니다.
* **로봇 파운데이션 모델:** '로봇에 활용되는 범용 사전학습 모델'이라는 조건만 만족하면 되므로 개념적으로 더 넓습니다.

#### 관련 용어 정리

1. **로보틱 트랜스포머 (Robotic Transformer)**
   * 2020년대 초반 일부 모델에서 사용되던 표현으로, 최근에는 로봇 파운데이션 모델과 거의 동의어로 사용됩니다.
2. **피지컬 AI (Physical AI)**
   * 가장 포괄적인 개념입니다.
   * 시연 데이터를 그대로 복사하는 Motion Retargeting, 특정 작업에 특화된 저일반화 AI까지도 포함합니다.

개념의 포함 관계는 다음과 같이 정리할 수 있습니다.

> **Physical AI ⊃ Robot Foundation Model ⊃ VLA**

### 3. 산업적 배경과 연구 주도 세력

현재 VLA 및 로봇 파운데이션 모델 연구는 \*\*구글 딥마인드(Google DeepMind)\*\*와 \*\*엔비디아(NVIDIA)\*\*가 주도하고 있습니다. 대부분의 로봇 하드웨어 기업들은 이들이 제공하는 플랫폼을 활용해 시뮬레이션, 학습, 모델 개발을 진행하는 구조를 택하고 있습니다.

#### 플랫폼 중심 분업 구조

엔비디아와 구글 모두 로봇 제조사와 직접 경쟁하기보다는 **플랫폼 사업자**로 자리매김하는 전략을 취하고 있습니다.

* 시뮬레이션
* 학습 인프라
* 파운데이션 모델 제공

이로 인해 **하드웨어 제조**와 **모델·시뮬레이션 플랫폼 개발** 사이의 분업화가 빠르게 진행되고 있습니다.

> **\[이미지** 삽입 구간]
>
> **NVIDIA / Google 중심 생태계 구조도** (로봇 제조사–플랫폼 분업 관계 그림)

### 4. 협력과 경쟁: MuJoCo, Newton, Warp

VLA 모델 개발은 각자 진행되고 있지만, 로봇 인공지능 분야는 구조적 한계를 공유하고 있습니다.

* **데이터 부족**
* **시뮬레이션** 인프라 **부족**

이로 인해 엔비디아와 구글은 경쟁 관계이면서도 다음과 같은 협력을 이어가고 있습니다.

* **Newton 물리 엔진 공동 개발**
* **MuJoCo Warp 공개 (GTC 2025):** MuJoCo를 NVIDIA GPU에 최적화

이는 로봇 AI 학습 인프라가 아직 성숙 단계에 도달하지 못했음을 보여주는 동시에, 기초 인프라 영역에서는 협력이 필수적임을 시사합니다.

> **\[이미지 삽입 구간]**
>
> **MuJoCo → MuJoCo Warp 구조** (CPU 기반 vs GPU 가속 시뮬레이션 비교)

### 5. 테슬라 옵티머스의 특수성

테슬라 옵티머스는 사용 중인 모델의 구체적인 기술 스택을 단 한 번도 공개한 적이 없습니다. 다만, 테슬라 차량에 적용된 **ADVA(Advanced Driver-Assistance)** 계열 모델과 유사한 접근이 사용된다는 정보만 알려져 있습니다.

엔비디아 수석 연구원 \*\*짐 팬(Jim Fan)\*\*은 다음과 같은 개인적인 추측을 남긴 바 있으나, 공식적인 확인은 없습니다.

* 옵티머스가 VLA 컴포넌트를 포함할 가능성
* 시연 데이터와 대규모 비디오 학습의 결합

### 6. 주요 개념 정리

#### 6.1 Sim2Real (Sim-to-Real Transfer)

시뮬레이션 환경에서 학습한 정책이나 모델을 실제 로봇에 적용하는 것을 의미합니다. 핵심 과제는 **Reality Gap**을 극복하는 것입니다. Sim2Real은 특히 강화학습 맥락에서 자주 언급되며, 이를 얼마나 잘 달성하느냐가 향후 인공지능 로봇 상용화의 성패를 가를 가능성이 높습니다.

#### 6.2 일반화 (Generalization)

학습 데이터에 포함되지 않았던 아래 상황들에 대해서도 로봇이 성공적으로 작업을 수행하는 능력을 의미합니다. 진정한 범용 로봇 지능을 위해 필수적인 요소입니다.

* 새로운 환경
* 새로운 객체
* 새로운 명령

#### 6.3 멀티모달 학습 (Multi-modal Learning)

텍스트, 이미지, 비디오, 오디오, 촉각, 관절 상태 등 여러 데이터 양식을 동시에 입력받아 통합적으로 학습하는 방식입니다. VLA 모델은 본질적으로 멀티모달 학습이 불가능하면 성립할 수 없습니다.

> **\[이미지 삽입 구간]**
>
> **멀티모달 입력 통합 구조**

#### 6.4 크로스 임바디먼트 (Cross-embodiment)

지능이 특정 로봇 하드웨어에 종속되지 않고 **다른 로봇으로 전이될 수 있는 특성**을 의미합니다. `Open X-Embodiment`와 같은 데이터셋은 이 개념을 전제로 설계되었습니다.

#### 6.5 액션 토큰 (Action Token)

로봇의 행동을 추상화·이산화한 단위로, 언어 모델의 토큰과 유사한 역할을 수행합니다. 다만 LLM의 _function callin&#x67;_&#xACFC;는 명확히 구분되며, 실제 물리 세계에서의 상호작용을 전제로 합니다.

### 7. 목적과 활용

#### 7.1 Control

* **Manipulation** (조작)
* **Locomotion** (이동)
* **Task Planning** (작업 계획)

초기에는 고정 베이스 조작에 국한되었으나, 최근에는 mobile manipulation, loco-manipulation까지 확장되고 있습니다.

#### 7.2 접근 방식 분류

| 구분                   | 설명                    | 대표 모델                          |
| -------------------- | --------------------- | ------------------------------ |
| **Modular Approach** | VLM·LLM 중심 서브모듈 결합 방식 | CLIPort, SayCan, PaLM-E        |
| **End-to-End VLA**   | 입력부터 액션까지 단일 정책으로 처리  | RT-1, RT-2, OpenVLA, Octo, GR1 |

### 8. 대표 모델 및 플랫폼

* **π₀ / π₀.₅**
  * Flow Matching 기반 VLA의 시초적 모델
* **Gemini Robotics**
  * Gemini 멀티모달 추론과 결합된 로봇 파운데이션 모델
* **Isaac GR00T**
  * 엔비디아의 로봇 AI 풀스택
  * 구성: Foundation Model (GR00T N1), Omniverse, Cosmos, Jetson Thor

> **\[이미지 삽입 구간]**
>
> **Isaac GR00T 전체 구성도**

### 9. 학습, 시뮬레이터, 월드 모델

_(이 섹션은 이후 MuJoCo + VLA 챕터로 자연스럽게 연결됩니다)_

### 마무리

VLA는 아직 완성된 기술이 아니라 **데이터, 시뮬레이션, 학습 알고리즘** 모두가 동시에 진화 중인 영역입니다.

이 GitBook에서는 이후 챕터에서 **MuJoCo + YOLO + LLM** 조합을 통해 VLA를 흉내내는 실험적 구조를 단계적으로 구현하며, 이론과 실제 사이의 간극을 직접 체험해 봅니다.
