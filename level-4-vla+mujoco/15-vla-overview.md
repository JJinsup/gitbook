---
description: From “Sim2Real” to Physical AI
---

# \[15] VLA Overview

### 0. LLM vs VLM vs VLA

### LLM · VLM · VLA 비교

<table><thead><tr><th width="117">구분</th><th width="199">LLM (Large Language Model)</th><th width="205">VLM (Vision-Language Model)</th><th>VLA (Vision-Language-Action Model)</th></tr></thead><tbody><tr><td>정의</td><td>언어를 이해하고 생성하는 모델</td><td>보고(vision) + 언어를 이해하는 모델</td><td>보고 + 이해하고 실제로 행동하는 로봇 모델</td></tr><tr><td>주요 입력</td><td>텍스트</td><td>이미지, 비디오, 텍스트</td><td>이미지, 비디오, 텍스트, 로봇 상태</td></tr><tr><td>주요 출력</td><td>텍스트</td><td>텍스트, 캡션, 임베딩</td><td>로봇 액션, 제어 명령, 궤적</td></tr><tr><td>다루는 세계</td><td>언어 세계</td><td>시각적 세계</td><td>물리 세계</td></tr><tr><td>물리적 행동</td><td>불가능</td><td>불가능</td><td>가능 (액추에이터 제어)</td></tr><tr><td>핵심 토큰</td><td>텍스트 토큰</td><td>텍스트 + 비전 토큰</td><td>텍스트 + 비전 + 액션 토큰</td></tr><tr><td>대표 예시</td><td>GPT, Claude, LLaMA</td><td>CLIP, BLIP, Flamingo, Gemini</td><td>RT-2, OpenVLA, Octo, GR00T</td></tr><tr><td>주 활용 분야</td><td>챗봇, 문서 생성, 코딩</td><td>이미지 이해, 로봇 인식</td><td>로봇 조작, 내비게이션, 제어</td></tr></tbody></table>

### 1. 개요

VLA 모델(Vision-Language-Action Model)은 텍스트, 이미지·비디오, 시연(Demonstration) 등의 입력을 받아 로봇의 액션을 직접 생성하는 로봇 파운데이션 모델을 통칭하는 개념입니다.

즉, 로봇 내부에서 **인지** → 추론 **→ 행동 생성**을 하나의 모델 또는 하나의 통합 파이프라인으로 수행하는 생성형 인공지능 로봇 모델이라 볼 수 있습니다.

기존의 로봇 제어 시스템이 아래와 같이 개별 모듈로 나누어 설계했다면,

* 센서 처리
* 상태 추정
* 경로 계획
* 제어기 설계

VLA는 이 중 상당 부분을 대규모 신경망 기반 모델이 통합적으로 담당한다는 점에서 근본적인 차이를 가집니다.

> **\[이미지 삽입 구간]**
>
> **Vision–Language–Action 파이프라인 개념도** (입력(이미지·텍스트) → 추론 → 로봇 행동 흐름 다이어그램)

### 2. VLA와 로봇 파운데이션 모델의 관계

VLA는 로봇 파운데이션 모델(Robot Foundation Model)의 **부분집합**에 해당합니다.

* **VLA:** 반드시 Vision, Language, Action이라는 세 가지 컴포넌트를 모두 포함해야 합니다.
* **로봇 파운데이션 모델:** '로봇에 활용되는 범용 사전학습 모델'이라는 조건만 만족하면 되므로 개념적으로 더 넓습니다.

#### 관련 용어 정리

1. **로보틱 트랜스포머 (Robotic Transformer)**
   * 2020년대 초반 일부 모델에서 사용되던 표현으로, 최근에는 로봇 파운데이션 모델과 거의 동의어로 사용됩니다.
2. **피지컬 AI (Physical AI)**
   * 가장 포괄적인 개념입니다.
   * 시연 데이터를 그대로 복사하는 Motion Retargeting, 특정 작업에 특화된 저일반화 AI까지도 포함합니다.

개념의 포함 관계는 다음과 같이 정리할 수 있습니다.

> **Physical AI ⊃ Robot Foundation Model ⊃ VLA**

### 3. 산업적 배경과 연구 주도 세력

현재 VLA 및 로봇 파운데이션 모델 연구는 구글 딥마인드와 엔비디아가 주도하고 있습니다. 대부분의 로봇 하드웨어 기업들은 이들이 제공하는 플랫폼을 활용해 시뮬레이션, 학습, 모델 개발을 진행하는 구조를 택하고 있습니다.

#### 플랫폼 중심 분업 구조

엔비디아와 구글 모두 로봇 제조사와 직접 경쟁하기보다는 **플랫폼 사업자**로 자리매김하는 전략을 취하고 있습니다.

* 시뮬레이션
* 학습 인프라
* 파운데이션 모델 제공

이로 인해 **하드웨어 제조**와 **모델·시뮬레이션 플랫폼 개발** 사이의 분업화가 빠르게 진행되고 있습니다.

> **\[이미지 삽입 구간]**
>
> **NVIDIA / Google 중심 생태계 구조도** (로봇 제조사–플랫폼 분업 관계 그림)

### 4. 협력과 경쟁: MuJoCo, Newton, Warp

VLA 모델 개발은 각자 진행되고 있지만, 로봇 인공지능 분야는 구조적 한계를 공유하고 있습니다.

* **데이터 부족**
* **시뮬레이션 인프라 부족**

이로 인해 엔비디아와 구글은 경쟁 관계이면서도 다음과 같은 협력을 이어가고 있습니다.

* **Newton 물리 엔진 공동 개발**
* **MuJoCo Warp 공개 (GTC 2025):** MuJoCo를 NVIDIA GPU에 최적화

> **\[이미지 삽입 구간]**
>
> **MuJoCo → MuJoCo Warp 구조** (CPU 기반 vs GPU 가속 시뮬레이션 비교)

### 5. 주요 개념 정리

#### 5.1 Sim2Real (Sim-to-Real Transfer)

시뮬레이션 환경에서 학습한 정책이나 모델을 실제 로봇에 적용하는 것을 의미합니다. 핵심 과제는 **Reality Gap**을 극복하는 것입니다. Sim2Real은 특히 강화학습 맥락에서 자주 언급되며, 이를 얼마나 잘 달성하느냐가 향후 인공지능 로봇 상용화의 성패를 가를 가능성이 높습니다.

#### 5.2 Generalization

학습 데이터에 포함되지 않았던 아래 상황들에 대해서도 로봇이 성공적으로 작업을 수행하는 능력을 의미합니다. 진정한 범용 로봇 지능을 위해 필수적인 요소입니다.

* 새로운 환경
* 새로운 객체
* 새로운 명령

#### 5.3 Multi-modal Learning

텍스트, 이미지, 비디오, 오디오, 촉각, 관절 상태 등 여러 데이터 양식을 동시에 입력받아 통합적으로 학습하는 방식입니다. VLA 모델은 본질적으로 멀티모달 학습이 불가능하면 성립할 수 없습니다.

> **\[이미지 삽입 구간]**
>
> **멀티모달 입력 통합 구조**

#### 5.4 Cross-embodiment

지능이 특정 로봇 하드웨어에 종속되지 않고 **다른 로봇으로 전이될 수 있는 특성**을 의미합니다. `Open X-Embodiment`와 같은 데이터셋은 이 개념을 전제로 설계되었습니다.

#### 5.5 Action Token

로봇의 행동을 추상화·이산화한 단위로, 언어 모델의 토큰과 유사한 역할을 수행합니다. 다만 LLM의 _function callin&#x67;_&#xACFC;는 명확히 구분되며, 실제 물리 세계에서의 상호작용을 전제로 합니다.

### 7. 목적과 활용

#### 7.1 Control

* **Manipulation** (조작)
* **Locomotion** (이동)
* **Task Planning** (작업 계획)

초기에는 고정 베이스 조작에 국한되었으나, 최근에는 mobile manipulation, loco-manipulation까지 확장되고 있습니다.

#### 7.2 접근 방식 분류

| 구분                   | 설명                    | 대표 모델                          |
| -------------------- | --------------------- | ------------------------------ |
| **Modular Approach** | VLM·LLM 중심 서브모듈 결합 방식 | CLIPort, SayCan, PaLM-E        |
| **End-to-End VLA**   | 입력부터 액션까지 단일 정책으로 처리  | RT-1, RT-2, OpenVLA, Octo, GR1 |

### 8. 대표 모델 및 플랫폼

<figure><img src="../.gitbook/assets/10.1088-1674-4926-25020034-Figure1.jpg" alt=""><figcaption></figcaption></figure>

The evolution timeline of robotics and embodied AI : [`출처`](https://www.jos.ac.cn/en/article/doi/10.1088/1674-4926/25020034?viewType=HTML)

* **π₀ / π₀.₅**
  * Flow Matching 기반 VLA의 시초적 모델
* **Gemini Robotics**
  * Gemini 멀티모달 추론과 결합된 로봇 파운데이션 모델
* **Isaac GR00T**
  * 엔비디아의 로봇 AI 풀스택
  * 구성: Foundation Model (GR00T N1), Omniverse, Cosmos, Jetson Thor

> **\[이미지 삽입 구간]**
>
> **Isaac GR00T 전체 구성도**

### 9. 학습, 시뮬레이터, 월드 모델

VLA 모델의 성능은 데이터의 양과 질, 그리고 이를 학습할 수 있는 환경(시뮬레이터)에 크게 의존합니다.

#### 9.1 데이터셋 (Dataset)

인공지능 로봇, 특히 실제 환경과 상호작용하며 범용적인 능력을 학습해야 하는 로봇 파운데이션 모델의 개발에 있어 양질의 대규모 데이터셋은 필수적입니다. 인터넷에서 쉽게 구할 수 있는 비전, 자연어 데이터와 달리 **액션 데이터**는 구축 비용이 매우 높고 대규모 확보가 어렵다는 특징이 있습니다.

현재는 체화 특정성(embodiment specificity)이 낮은 시뮬레이션 합성 데이터나 웹상의 영상 데이터를 병행하여 학습하는 추세입니다.

**주요 대규모 데이터셋 프로젝트**

* **Robomimic:** 스탠퍼드 대학교(SVL) 주도로 개발된 로봇 모방 학습 벤치마크 프레임워크입니다. 사람의 원격 조종(Teleoperation) 데이터 등 다양한 품질의 시연 데이터를 제공하여 알고리즘 성능을 공정하게 비교할 수 있게 합니다.
* **Open X-Embodiment:** 구글 딥마인드와 스탠퍼드 등 30여 개 기관이 협력한 초대형 프로젝트입니다. 22종의 서로 다른 로봇(Cross-embodiment) 데이터를 통합하여 일반화 성능을 극대화하는 것을 목표로 하며, **RT-X**와 **Octo**의 학습에 사용되었습니다.
* **Bridge Data:** UC 버클리와 구글이 주도한 데이터셋으로, 저비용 로봇 팔을 이용해 수집된 일상 사물 조작(Manipulation) 데이터가 주를 이룹니다.
* **DROID (Distributed Robot Interaction Dataset):** 2024년 뉴욕 대학교(NYU) 주도로 공개되었습니다. 전 세계 자원자들이 스마트폰과 로봇 팔(WidowX)을 이용해 분산 수집한 데이터로, 저비용으로 대규모 데이터를 구축할 수 있는 가능성을 보여주었습니다.

#### 9.2 시뮬레이터

물리 인공지능 학습을 위해 다양한 가상 환경이 활용됩니다. 특히 2025년에는 구글, 엔비디아, 디즈니 리서치가 공동 개발한 로보틱스 전용 물리엔진 뉴턴(Newton)이 발표되어 기존 물리 엔진(PhysX 등)을 대체할 예정입니다.

**주요 시뮬레이션 플랫폼**

* **Isaac Sim / Isaac Lab:** 엔비디아의 시뮬레이션 플랫폼으로, 모방학습과 강화학습을 모두 지원하며 고성능 GPU 가속을 활용합니다.
* **MuJoCo (Multi-Joint dynamics with Contact):** 구글 딥마인드가 관리하는 고성능 물리 엔진입니다. 정밀한 물리 연산이 강점입니다.
* **OpenAI Gym (Gymnasium):** 강화학습 연구의 입문과도 같은 환경으로, 간단한 제어부터 로봇 시뮬레이션까지 다양한 환경을 제공합니다.
* **Project Malmo / CraftGround:** 마인크래프트 기반의 복잡한 3D 환경 학습 플랫폼입니다.
* **기타:** DeepMind Lab, Unity ML-Agents, VizDoom 등이 있습니다.

#### 9.3 월드 모델 (World Model)

기존의 물리 시뮬레이터(Isaac Sim, MuJoCo 등)는 강체 역학(Rigid body dynamics) 시뮬레이션에 최적화되어 있어, 현실 세계의 복잡한 물리 법칙과 상호작용을 완벽히 모사하기 어렵다는 한계가 있습니다.

이를 극복하기 위해 **인공 신경망 기반의 월드 모델**이 도입되고 있습니다.

* **NVIDIA Cosmos:** 엔비디아가 주도하는 로봇 학습용 월드 모델입니다.
* **기업별 독자 개발:** 1x 등 일부 로봇 플랫폼 기업은 자사 하드웨어에 최적화된 월드 모델을 자체 개발하기도 합니다.

#### 9.4 학습 알고리즘 (Learning Algorithms)

**9.4.1 강화학습 (Reinforcement Learning, RL)**

로봇이 시행착오를 통해 최적의 행동 정책(Policy)을 스스로 찾아가는 방식입니다. 행동에 대한 보상(Reward)과 벌칙(Penalty)을 통해 학습합니다.

* **장점:** 미리 프로그래밍되지 않은 상황에서도 적응 가능하며, 데이터가 없는 상태에서도 시뮬레이션을 통해 학습할 수 있습니다.
* **한계:** 동작이 다소 어색하거나 끊길 수 있으며(Sim-to-Real Gap), 초기 학습 비용이 높습니다.

**9.4.2 모방학습 (Imitation Learning, IL)**

전문가(사람)의 시연(Demonstration) 데이터를 로봇이 그대로 흉내 내도록 학습하는 방식입니다. 모방학습은 가이드라인을 제시하는 강화학습의 지도학습 버전이라고 할 수 있는 **VLA 학습의 핵심**입니다.

* **행동 복제 (Behavior Cloning, BC):** 전문가의 '상태-행동' 쌍을 지도 학습(Supervised Learning) 방식으로 학습합니다.
* **장점:** 사람처럼 유려하고 자연스러운 동작을 구현할 수 있습니다.
* **한계:** 양질의 시연 데이터를 대규모로 확보하는 것이 어렵습니다.

### 마무리

VLA는 아직 완성된 기술이 아니라 **데이터, 시뮬레이션, 학습 알고리즘** 모두가 동시에 진화 중인 영역입니다.

이 GitBook에서는 이후 챕터에서 **MuJoCo + YOLO + LLM** 조합을 통해 VLA를 흉내내는 실험적 구조를 단계적으로 구현하며, 이론과 실제 사이의 간극을 직접 체험해 봅니다.
